朴素贝叶斯分类（Naive Bayes classification）是一种基于贝叶斯定理的分类算法。它假设特征之间相互独立，因此可以简化计算。在朴素贝叶斯分类中，我们根据给定的特征向量，计算每个可能的类别的概率，并选择概率最大的类别作为预测结果。

具体地说，朴素贝叶斯分类中的贝叶斯定理可以表示为：

$$
P(c|\boldsymbol x) = \frac{P(c)P(\boldsymbol x|c)}{P(\boldsymbol x)}
$$

其中，$P(c|\boldsymbol x)$ 表示在给定特征向量 $\boldsymbol x$ 的条件下，类别 $c$ 的概率；$P(c)$ 表示类别 $c$ 的先验概率；$P(\boldsymbol x|c)$ 表示在类别 $y$ 的条件下，特征向量 $\boldsymbol x$ 的联合概率；$P(\boldsymbol x)$ 表示特征向量 $\boldsymbol x$ 的先验概率。

由于特征之间相互独立，所以可以将 $P(\boldsymbol x|c)$ 表示为各个特征的条件概率的乘积：

$$
P(\boldsymbol x|c) = P(x_1|c)P(x_2|c)...P(x_n|c)
$$

因此，朴素贝叶斯分类中的贝叶斯定理可以进一步简化为：

$$
P(c|\boldsymbol x) = \frac{P(c)\prod_{i=1}^n P(x_i|c)}{P(\boldsymbol x)}
$$

由于分母 $P(\boldsymbol x)$ 对于所有类别都是相同的，所以可以不考虑它，只比较分子的大小，选择概率最大的类别作为预测结果。

1. 贝叶斯分类器通过**估计样本和类别的联合概率**间接地获得样本的后验概率，因此它是一个**生成式概率模型。**

2. 贝叶斯决策论（Bayesian decision theory）是在概率框架下实施决策的基本方法。 在分类问题情况下，在所有相关概率都已知的理想情形下，贝叶斯决策考虑如何基于这些概率和误判损失来选择最优的类别标记。

3. 贝叶斯判定准则（Bayes decision rule）： 为最小化总体风险R(y^=h(x)|x)，只需在每个样本上选择那个能使条件风险R(y=c|x)最小的类别标记c*, 即y^=h*(x)=c*=arg min{R(y=c*|x)} , 此时，y^=h*(x)被称为贝叶斯最优分类器(Bayes optimal classifier)，与之对应的总体风险R(y^=h*(x)|x) 称为贝叶斯风险 (Bayes risk)。

4. 由于条件风险R(c|x)=1-P(x|c), 最小化分类错误率的贝叶斯最优分类器为y^=h*(x)=c*=argmax{ P(c|x) }, 即对每个样本x，贝叶斯最优分类器选择能使后验概率 P(c|x)最大的类别标记c*。

5. 使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P(c|x)。基于有限的训练样本尽可能准确地估计出后验概率P(c|x)，可以先对联合概率P(x, c)建模，再通过估计类别先验概率P(c)和类条件概率(x|c)获得后验概率P(c|x)=P(x,c)/P(x)=P(x|c)P(c)/P(x)。

6. 类条件概率或似然P(x|c)的常用估计策略：(1) 参数化方法：先假定其概率分布具有某种参数确定的函数形式，再基于训练样本通过最大化后验概率对概率分布参数进行优化求解；(2) 非参数化方法：给定类别c，用落在连续输入值间隔内或离散输入值处的样本数目占总样本数目的比例估计P(x|c)。 

7. 在贝叶斯分类器语境下，类条件概率P(x|c)被称为似然(likelyhood)。由此，给定类先验概率P(c), 最大化后验概率P(c|x)转化为极大化似然P(x|c)。

8. 极大似然估计是试图在参数所有可能的取值中，找到一个使数据出现的“可能性”最大的参数值。

9. 有参数的类条件概率模型的训练过程就是参数估计过程，统计学界的两个学派提供了不同的方案： 频率主义学派 (frequentist)和贝叶斯学派 (Bayesian)。极大似然估计是频率主义学派的参数估计方法。

10. 估计后验概率P(c|x)的主要困难：类条件概率或似然P(x|c)是样本x的所有特征、维度或属性上的联合概率，难以从有限的训练样本估计获得。

11. 朴素贝叶斯分类器(Naïve Bayes Classifier)采用了“属性条件独立性假设”(attribute conditional independence assumption)：每个属性独立地对分类结果发生影响。