逻辑回归使用的是最大似然估计来寻找最优的模型参数，其常用的优化算法是梯度下降法。梯度下降法的核心思想是通过迭代来更新模型参数，使损失函数最小化。
则Logistic回归的目标函数为：

$$  
\min_{w,b}\ J(w,b)=-\frac{1}{n}\sum_{i=1}^{n}[y_i\log(\hat{y_i})+(1-y_i)\log(1-\hat{y_i})]+\frac{\lambda}{2}||w||^2  
$$

其中，$w\in R^d$为模型的权重向量，$b$为偏置项，$\hat{y_i}$为第$i$个样本的预测值，$||w||^2$为L2正则化项，$\lambda$为正则化系数。

$\hat{y_i}$表示第$i$个样本属于正类的概率，可以表示为：

$$  
\hat{y_i}=\frac{1}{1+e^{-z_i}}  
$$

其中，$z_i=w^Tx_i+b$为第$i$个样本的线性预测值。

Logistic回归的目标是最小化损失函数$J(w,b)$，其中包括交叉熵损失和L2正则化项。交叉熵损失函数可以度量模型预测值与实际标签之间的差异，L2正则化项可以惩罚模型的复杂度，避免过拟合的发生。通过调整正则化系数$\lambda$的值，可以平衡模型的拟合程度和泛化能力。

具体来说，逻辑回归使用的是交叉熵损失函数，其公式如下：
$$L(\boldsymbol{\theta})=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\log h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})+(1-y^{(i)})\log(1-h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})))$$

其中，$y^{(i)}$是第$i$个样本的真实标签，$\mathbf{x}^{(i)}$是第$i$个样本的特征向量，$h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})$是逻辑回归模型的预测值。

梯度下降法的核心是计算损失函数$L(\boldsymbol{\theta})$对模型参数$\boldsymbol{\theta}$的偏导数，然后根据偏导数更新模型参数。偏导数的计算公式如下：

$$\frac{\partial L(\boldsymbol{\theta})}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^{m}(h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}$$

其中，$j$是模型参数的下标。

更新模型参数的公式如下：

$$\theta_j=\theta_j-\alpha\frac{\partial L(\boldsymbol{\theta})}{\partial \theta_j}$$

其中，$\alpha$是学习率，控制模型参数更新的步长。

在实际应用中，我们通常会使用批量梯度下降（Batch Gradient Descent）或随机梯度下降（Stochastic Gradient Descent）来更新模型参数。批量梯度下降每次使用所有样本来更新模型参数，而随机梯度下降每次只使用一个样本来更新模型参数。随机梯度下降可以加速模型的收敛速度，但会引入一定的随机性。

因此，逻辑回归使用梯度下降法来最小化交叉熵损失函数，从而寻找最优的模型参数。在每次迭代中，计算损失函数对模型参数的偏导数，并根据偏导数更新模型参数，使损失函数逐步减少，最终达到收敛。