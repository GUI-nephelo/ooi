逻辑回归使用的是最大似然估计来寻找最优的模型参数，其常用的优化算法是梯度下降法。梯度下降法的核心思想是通过迭代来更新模型参数，使损失函数最小化。

具体来说，逻辑回归使用的是交叉熵损失函数，其公式如下：

$$L(\boldsymbol{\theta})=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\log h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})+(1-y^{(i)})\log(1-h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})))$$

其中，$y^{(i)}$是第$i$个样本的真实标签，$\mathbf{x}^{(i)}$是第$i$个样本的特征向量，$h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})$是逻辑回归模型的预测值。

梯度下降法的核心是计算损失函数$L(\boldsymbol{\theta})$对模型参数$\boldsymbol{\theta}$的偏导数，然后根据偏导数更新模型参数。偏导数的计算公式如下：

$$\frac{\partial L(\boldsymbol{\theta})}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^{m}(h_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}$$

其中，$j$是模型参数的下标。

更新模型参数的公式如下：

$$\theta_j=\theta_j-\alpha\frac{\partial L(\boldsymbol{\theta})}{\partial \theta_j}$$

其中，$\alpha$是学习率，控制模型参数更新的步长。

在实际应用中，我们通常会使用批量梯度下降（Batch Gradient Descent）或随机梯度下降（Stochastic Gradient Descent）来更新模型参数。批量梯度下降每次使用所有样本来更新模型参数，而随机梯度下降每次只使用一个样本来更新模型参数。随机梯度下降可以加速模型的收敛速度，但会引入一定的随机性。

因此，逻辑回归使用梯度下降法来最小化交叉熵损失函数，从而寻找最优的模型参数。在每次迭代中，计算损失函数对模型参数的偏导数，并根据偏导数更新模型参数，使损失函数逐步减少，最终达到收敛。